[INFO|2025-08-23 20:08:11] tokenization_utils_base.py:2065 >> loading file merges.txt
[INFO|2025-08-23 20:08:11] tokenization_utils_base.py:2065 >> loading file tokenizer.json
[INFO|2025-08-23 20:08:11] tokenization_utils_base.py:2065 >> loading file added_tokens.json
[INFO|2025-08-23 20:08:11] tokenization_utils_base.py:2065 >> loading file special_tokens_map.json
[INFO|2025-08-23 20:08:11] tokenization_utils_base.py:2065 >> loading file tokenizer_config.json
[INFO|2025-08-23 20:08:11] tokenization_utils_base.py:2065 >> loading file chat_template.jinja
[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2336 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-08-23 20:08:12] image_processing_base.py:376 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/preprocessor_config.json
[INFO|2025-08-23 20:08:12] image_processing_base.py:376 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/preprocessor_config.json
[INFO|2025-08-23 20:08:12] image_processing_base.py:423 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2065 >> loading file vocab.json
[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2065 >> loading file merges.txt
[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2065 >> loading file tokenizer.json
[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2065 >> loading file added_tokens.json
[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2065 >> loading file special_tokens_map.json
[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2065 >> loading file tokenizer_config.json
[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2065 >> loading file chat_template.jinja
[INFO|2025-08-23 20:08:12] tokenization_utils_base.py:2336 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|2025-08-23 20:08:12] logging.py:328 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|2025-08-23 20:08:12] video_processing_utils.py:718 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/preprocessor_config.json
[WARNING|2025-08-23 20:08:12] logging.py:328 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[WARNING|2025-08-23 20:08:12] logging.py:328 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[WARNING|2025-08-23 20:08:12] logging.py:328 >> You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
[INFO|2025-08-23 20:08:12] video_processing_utils.py:764 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|2025-08-23 20:08:13] processing_utils.py:1117 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "size_divisor": null,
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-08-23 20:08:13] logging.py:143 >> Loading dataset finetune_data.json...
[INFO|2025-08-23 20:08:15] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 20:08:15] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 20:08:15] logging.py:143 >> KV cache is disabled during training.
[INFO|2025-08-23 20:08:15] modeling_utils.py:1305 >> loading weights file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/model.safetensors.index.json
[INFO|2025-08-23 20:08:15] modeling_utils.py:2411 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|2025-08-23 20:08:15] configuration_utils.py:1098 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[INFO|2025-08-23 20:08:15] modeling_utils.py:2411 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|2025-08-23 20:08:15] modeling_utils.py:2411 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
[INFO|2025-08-23 20:08:22] modeling_utils.py:5606 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.

[INFO|2025-08-23 20:08:22] modeling_utils.py:5614 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
[INFO|2025-08-23 20:08:22] configuration_utils.py:1051 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/generation_config.json
[INFO|2025-08-23 20:08:22] configuration_utils.py:1098 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|2025-08-23 20:08:22] logging.py:143 >> Gradient checkpointing enabled.
[INFO|2025-08-23 20:08:22] logging.py:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-23 20:08:22] logging.py:143 >> Upcasting trainable params to float32.
[INFO|2025-08-23 20:08:22] logging.py:143 >> Fine-tuning method: LoRA
[INFO|2025-08-23 20:08:22] logging.py:143 >> Found linear modules: k_proj,o_proj,v_proj,down_proj,gate_proj,q_proj,up_proj
[INFO|2025-08-23 20:08:22] logging.py:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].
[INFO|2025-08-23 20:08:22] logging.py:143 >> Set multi model projector not trainable: visual.merger.
[INFO|2025-08-23 20:08:24] logging.py:143 >> trainable params: 161,480,704 || all params: 8,453,647,360 || trainable%: 1.9102
[INFO|2025-08-23 20:08:24] trainer.py:757 >> Using auto half precision backend
[INFO|2025-08-23 20:08:25] trainer.py:2433 >> ***** Running training *****
[INFO|2025-08-23 20:08:25] trainer.py:2434 >>   Num examples = 2,296
[INFO|2025-08-23 20:08:25] trainer.py:2435 >>   Num Epochs = 9
[INFO|2025-08-23 20:08:25] trainer.py:2436 >>   Instantaneous batch size per device = 1
[INFO|2025-08-23 20:08:25] trainer.py:2439 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|2025-08-23 20:08:25] trainer.py:2440 >>   Gradient Accumulation steps = 8
[INFO|2025-08-23 20:08:25] trainer.py:2441 >>   Total optimization steps = 648
[INFO|2025-08-23 20:08:25] trainer.py:2442 >>   Number of trainable parameters = 161,480,704
[WARNING|2025-08-23 20:08:30] logging.py:328 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
[WARNING|2025-08-23 20:08:30] logging.py:328 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
[WARNING|2025-08-23 20:08:30] logging.py:328 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
[WARNING|2025-08-23 20:08:30] logging.py:328 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
[INFO|2025-08-23 20:09:08] logging.py:143 >> {'loss': 1.6363, 'learning_rate': 4.9995e-05, 'epoch': 0.07, 'throughput': 4652.25}
[INFO|2025-08-23 20:09:47] logging.py:143 >> {'loss': 1.4097, 'learning_rate': 4.9976e-05, 'epoch': 0.14, 'throughput': 4716.07}
[INFO|2025-08-23 20:10:28] logging.py:143 >> {'loss': 1.3408, 'learning_rate': 4.9942e-05, 'epoch': 0.21, 'throughput': 4681.87}
[INFO|2025-08-23 20:11:08] logging.py:143 >> {'loss': 1.3100, 'learning_rate': 4.9894e-05, 'epoch': 0.28, 'throughput': 4702.14}
[INFO|2025-08-23 20:11:48] logging.py:143 >> {'loss': 1.2846, 'learning_rate': 4.9831e-05, 'epoch': 0.35, 'throughput': 4685.36}
[INFO|2025-08-23 20:12:30] logging.py:143 >> {'loss': 1.2698, 'learning_rate': 4.9753e-05, 'epoch': 0.42, 'throughput': 4675.34}
[INFO|2025-08-23 20:13:09] logging.py:143 >> {'loss': 1.2612, 'learning_rate': 4.9661e-05, 'epoch': 0.49, 'throughput': 4690.29}
[INFO|2025-08-23 20:13:49] logging.py:143 >> {'loss': 1.2426, 'learning_rate': 4.9554e-05, 'epoch': 0.56, 'throughput': 4702.78}
[INFO|2025-08-23 20:14:29] logging.py:143 >> {'loss': 1.2454, 'learning_rate': 4.9433e-05, 'epoch': 0.63, 'throughput': 4699.30}
[INFO|2025-08-23 20:15:10] logging.py:143 >> {'loss': 1.2183, 'learning_rate': 4.9298e-05, 'epoch': 0.70, 'throughput': 4684.32}
[INFO|2025-08-23 20:15:51] logging.py:143 >> {'loss': 1.2211, 'learning_rate': 4.9148e-05, 'epoch': 0.77, 'throughput': 4685.27}
[INFO|2025-08-23 20:16:31] logging.py:143 >> {'loss': 1.2049, 'learning_rate': 4.8984e-05, 'epoch': 0.84, 'throughput': 4687.89}
[INFO|2025-08-23 20:17:11] logging.py:143 >> {'loss': 1.1720, 'learning_rate': 4.8806e-05, 'epoch': 0.91, 'throughput': 4689.42}
[INFO|2025-08-23 20:17:51] logging.py:143 >> {'loss': 1.1961, 'learning_rate': 4.8614e-05, 'epoch': 0.98, 'throughput': 4688.71}
[INFO|2025-08-23 20:18:30] logging.py:143 >> {'loss': 1.1531, 'learning_rate': 4.8408e-05, 'epoch': 1.04, 'throughput': 4684.64}
[INFO|2025-08-23 20:19:10] logging.py:143 >> {'loss': 1.1279, 'learning_rate': 4.8189e-05, 'epoch': 1.11, 'throughput': 4690.15}
[INFO|2025-08-23 20:19:49] logging.py:143 >> {'loss': 1.1222, 'learning_rate': 4.7955e-05, 'epoch': 1.18, 'throughput': 4696.82}
[INFO|2025-08-23 20:20:29] logging.py:143 >> {'loss': 1.0972, 'learning_rate': 4.7709e-05, 'epoch': 1.25, 'throughput': 4696.52}
[INFO|2025-08-23 20:21:09] logging.py:143 >> {'loss': 1.1285, 'learning_rate': 4.7449e-05, 'epoch': 1.32, 'throughput': 4697.84}
[INFO|2025-08-23 20:21:49] logging.py:143 >> {'loss': 1.0942, 'learning_rate': 4.7175e-05, 'epoch': 1.39, 'throughput': 4699.89}
[INFO|2025-08-23 20:21:49] trainer.py:4408 >> 
***** Running Evaluation *****
[INFO|2025-08-23 20:21:49] trainer.py:4410 >>   Num examples = 24
[INFO|2025-08-23 20:21:49] trainer.py:4413 >>   Batch size = 1
[INFO|2025-08-23 20:21:53] trainer.py:4074 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100
[INFO|2025-08-23 20:21:53] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 20:21:53] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 20:21:54] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/chat_template.jinja
[INFO|2025-08-23 20:21:54] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/tokenizer_config.json
[INFO|2025-08-23 20:21:54] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/special_tokens_map.json
[INFO|2025-08-23 20:21:55] image_processing_base.py:258 >> Image processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/preprocessor_config.json
[INFO|2025-08-23 20:21:55] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/chat_template.jinja
[INFO|2025-08-23 20:21:56] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/tokenizer_config.json
[INFO|2025-08-23 20:21:56] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/special_tokens_map.json
[INFO|2025-08-23 20:21:56] video_processing_utils.py:582 >> Video processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/video_preprocessor_config.json
[INFO|2025-08-23 20:21:56] processing_utils.py:745 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-100/chat_template.jinja
[INFO|2025-08-23 20:22:35] logging.py:143 >> {'loss': 1.0905, 'learning_rate': 4.6889e-05, 'epoch': 1.46, 'throughput': 4669.68}
[INFO|2025-08-23 20:23:15] logging.py:143 >> {'loss': 1.0842, 'learning_rate': 4.6590e-05, 'epoch': 1.53, 'throughput': 4674.90}
[INFO|2025-08-23 20:23:54] logging.py:143 >> {'loss': 1.1031, 'learning_rate': 4.6278e-05, 'epoch': 1.60, 'throughput': 4677.46}
[INFO|2025-08-23 20:24:36] logging.py:143 >> {'loss': 1.0813, 'learning_rate': 4.5954e-05, 'epoch': 1.67, 'throughput': 4671.37}
[INFO|2025-08-23 20:25:16] logging.py:143 >> {'loss': 1.0600, 'learning_rate': 4.5617e-05, 'epoch': 1.74, 'throughput': 4674.22}
[INFO|2025-08-23 20:25:57] logging.py:143 >> {'loss': 1.0923, 'learning_rate': 4.5268e-05, 'epoch': 1.81, 'throughput': 4672.44}
[INFO|2025-08-23 20:26:37] logging.py:143 >> {'loss': 1.0681, 'learning_rate': 4.4907e-05, 'epoch': 1.88, 'throughput': 4672.95}
[INFO|2025-08-23 20:27:18] logging.py:143 >> {'loss': 1.0897, 'learning_rate': 4.4535e-05, 'epoch': 1.95, 'throughput': 4671.74}
[INFO|2025-08-23 20:27:55] logging.py:143 >> {'loss': 1.0546, 'learning_rate': 4.4151e-05, 'epoch': 2.01, 'throughput': 4675.66}
[INFO|2025-08-23 20:28:35] logging.py:143 >> {'loss': 0.9892, 'learning_rate': 4.3756e-05, 'epoch': 2.08, 'throughput': 4676.72}
[INFO|2025-08-23 20:29:16] logging.py:143 >> {'loss': 0.9678, 'learning_rate': 4.3350e-05, 'epoch': 2.15, 'throughput': 4678.41}
[INFO|2025-08-23 20:29:56] logging.py:143 >> {'loss': 0.9906, 'learning_rate': 4.2933e-05, 'epoch': 2.22, 'throughput': 4677.52}
[INFO|2025-08-23 20:30:37] logging.py:143 >> {'loss': 0.9562, 'learning_rate': 4.2505e-05, 'epoch': 2.29, 'throughput': 4679.31}
[INFO|2025-08-23 20:31:16] logging.py:143 >> {'loss': 0.9802, 'learning_rate': 4.2068e-05, 'epoch': 2.36, 'throughput': 4682.39}
[INFO|2025-08-23 20:31:57] logging.py:143 >> {'loss': 0.9647, 'learning_rate': 4.1620e-05, 'epoch': 2.43, 'throughput': 4679.82}
[INFO|2025-08-23 20:32:38] logging.py:143 >> {'loss': 0.9927, 'learning_rate': 4.1162e-05, 'epoch': 2.50, 'throughput': 4678.74}
[INFO|2025-08-23 20:33:19] logging.py:143 >> {'loss': 0.9677, 'learning_rate': 4.0695e-05, 'epoch': 2.57, 'throughput': 4675.94}
[INFO|2025-08-23 20:33:59] logging.py:143 >> {'loss': 0.9923, 'learning_rate': 4.0219e-05, 'epoch': 2.64, 'throughput': 4676.89}
[INFO|2025-08-23 20:34:41] logging.py:143 >> {'loss': 0.9936, 'learning_rate': 3.9734e-05, 'epoch': 2.71, 'throughput': 4673.90}
[INFO|2025-08-23 20:35:21] logging.py:143 >> {'loss': 0.9610, 'learning_rate': 3.9240e-05, 'epoch': 2.78, 'throughput': 4677.14}
[INFO|2025-08-23 20:35:21] trainer.py:4408 >> 
***** Running Evaluation *****
[INFO|2025-08-23 20:35:21] trainer.py:4410 >>   Num examples = 24
[INFO|2025-08-23 20:35:21] trainer.py:4413 >>   Batch size = 1
[INFO|2025-08-23 20:35:24] trainer.py:4074 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200
[INFO|2025-08-23 20:35:24] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 20:35:24] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 20:35:25] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/chat_template.jinja
[INFO|2025-08-23 20:35:25] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/tokenizer_config.json
[INFO|2025-08-23 20:35:25] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/special_tokens_map.json
[INFO|2025-08-23 20:35:27] image_processing_base.py:258 >> Image processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/preprocessor_config.json
[INFO|2025-08-23 20:35:27] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/chat_template.jinja
[INFO|2025-08-23 20:35:27] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/tokenizer_config.json
[INFO|2025-08-23 20:35:27] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/special_tokens_map.json
[INFO|2025-08-23 20:35:27] video_processing_utils.py:582 >> Video processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/video_preprocessor_config.json
[INFO|2025-08-23 20:35:27] processing_utils.py:745 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-200/chat_template.jinja
[INFO|2025-08-23 20:36:06] logging.py:143 >> {'loss': 0.9776, 'learning_rate': 3.8738e-05, 'epoch': 2.85, 'throughput': 4661.36}
[INFO|2025-08-23 20:36:47] logging.py:143 >> {'loss': 0.9546, 'learning_rate': 3.8227e-05, 'epoch': 2.92, 'throughput': 4660.68}
[INFO|2025-08-23 20:37:26] logging.py:143 >> {'loss': 0.9687, 'learning_rate': 3.7709e-05, 'epoch': 2.99, 'throughput': 4661.22}
[INFO|2025-08-23 20:38:03] logging.py:143 >> {'loss': 0.8659, 'learning_rate': 3.7184e-05, 'epoch': 3.06, 'throughput': 4664.57}
[INFO|2025-08-23 20:38:44] logging.py:143 >> {'loss': 0.8513, 'learning_rate': 3.6651e-05, 'epoch': 3.13, 'throughput': 4665.75}
[INFO|2025-08-23 20:39:24] logging.py:143 >> {'loss': 0.8470, 'learning_rate': 3.6112e-05, 'epoch': 3.20, 'throughput': 4665.70}
[INFO|2025-08-23 20:40:05] logging.py:143 >> {'loss': 0.8437, 'learning_rate': 3.5565e-05, 'epoch': 3.26, 'throughput': 4667.65}
[INFO|2025-08-23 20:40:45] logging.py:143 >> {'loss': 0.8488, 'learning_rate': 3.5013e-05, 'epoch': 3.33, 'throughput': 4667.41}
[INFO|2025-08-23 20:41:27] logging.py:143 >> {'loss': 0.8455, 'learning_rate': 3.4455e-05, 'epoch': 3.40, 'throughput': 4665.53}
[INFO|2025-08-23 20:42:07] logging.py:143 >> {'loss': 0.8703, 'learning_rate': 3.3891e-05, 'epoch': 3.47, 'throughput': 4666.36}
[INFO|2025-08-23 20:42:47] logging.py:143 >> {'loss': 0.8650, 'learning_rate': 3.3322e-05, 'epoch': 3.54, 'throughput': 4667.91}
[INFO|2025-08-23 20:43:26] logging.py:143 >> {'loss': 0.8344, 'learning_rate': 3.2748e-05, 'epoch': 3.61, 'throughput': 4669.50}
[INFO|2025-08-23 20:44:08] logging.py:143 >> {'loss': 0.8671, 'learning_rate': 3.2170e-05, 'epoch': 3.68, 'throughput': 4667.59}
[INFO|2025-08-23 20:44:48] logging.py:143 >> {'loss': 0.8813, 'learning_rate': 3.1587e-05, 'epoch': 3.75, 'throughput': 4666.57}
[INFO|2025-08-23 20:45:30] logging.py:143 >> {'loss': 0.8730, 'learning_rate': 3.1001e-05, 'epoch': 3.82, 'throughput': 4663.29}
[INFO|2025-08-23 20:46:11] logging.py:143 >> {'loss': 0.8649, 'learning_rate': 3.0411e-05, 'epoch': 3.89, 'throughput': 4663.07}
[INFO|2025-08-23 20:46:51] logging.py:143 >> {'loss': 0.8861, 'learning_rate': 2.9818e-05, 'epoch': 3.96, 'throughput': 4664.98}
[INFO|2025-08-23 20:47:28] logging.py:143 >> {'loss': 0.8113, 'learning_rate': 2.9222e-05, 'epoch': 4.03, 'throughput': 4666.37}
[INFO|2025-08-23 20:48:08] logging.py:143 >> {'loss': 0.7139, 'learning_rate': 2.8623e-05, 'epoch': 4.10, 'throughput': 4667.35}
[INFO|2025-08-23 20:48:50] logging.py:143 >> {'loss': 0.7284, 'learning_rate': 2.8023e-05, 'epoch': 4.17, 'throughput': 4664.65}
[INFO|2025-08-23 20:48:50] trainer.py:4408 >> 
***** Running Evaluation *****
[INFO|2025-08-23 20:48:50] trainer.py:4410 >>   Num examples = 24
[INFO|2025-08-23 20:48:50] trainer.py:4413 >>   Batch size = 1
[INFO|2025-08-23 20:48:53] trainer.py:4074 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300
[INFO|2025-08-23 20:48:53] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 20:48:53] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 20:48:54] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/chat_template.jinja
[INFO|2025-08-23 20:48:54] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/tokenizer_config.json
[INFO|2025-08-23 20:48:54] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/special_tokens_map.json
[INFO|2025-08-23 20:48:56] image_processing_base.py:258 >> Image processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/preprocessor_config.json
[INFO|2025-08-23 20:48:56] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/chat_template.jinja
[INFO|2025-08-23 20:48:56] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/tokenizer_config.json
[INFO|2025-08-23 20:48:56] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/special_tokens_map.json
[INFO|2025-08-23 20:48:56] video_processing_utils.py:582 >> Video processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/video_preprocessor_config.json
[INFO|2025-08-23 20:48:57] processing_utils.py:745 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-300/chat_template.jinja
[INFO|2025-08-23 20:49:37] logging.py:143 >> {'loss': 0.7374, 'learning_rate': 2.7420e-05, 'epoch': 4.24, 'throughput': 4651.74}
[INFO|2025-08-23 20:50:18] logging.py:143 >> {'loss': 0.7326, 'learning_rate': 2.6816e-05, 'epoch': 4.31, 'throughput': 4651.02}
[INFO|2025-08-23 20:50:59] logging.py:143 >> {'loss': 0.7260, 'learning_rate': 2.6212e-05, 'epoch': 4.38, 'throughput': 4650.57}
[INFO|2025-08-23 20:51:40] logging.py:143 >> {'loss': 0.7465, 'learning_rate': 2.5606e-05, 'epoch': 4.45, 'throughput': 4649.40}
[INFO|2025-08-23 20:52:21] logging.py:143 >> {'loss': 0.7345, 'learning_rate': 2.5000e-05, 'epoch': 4.52, 'throughput': 4647.99}
[INFO|2025-08-23 20:53:03] logging.py:143 >> {'loss': 0.7398, 'learning_rate': 2.4394e-05, 'epoch': 4.59, 'throughput': 4646.87}
[INFO|2025-08-23 20:53:45] logging.py:143 >> {'loss': 0.7517, 'learning_rate': 2.3788e-05, 'epoch': 4.66, 'throughput': 4646.42}
[INFO|2025-08-23 20:54:26] logging.py:143 >> {'loss': 0.7408, 'learning_rate': 2.3184e-05, 'epoch': 4.72, 'throughput': 4644.42}
[INFO|2025-08-23 20:55:06] logging.py:143 >> {'loss': 0.7587, 'learning_rate': 2.2580e-05, 'epoch': 4.79, 'throughput': 4644.50}
[INFO|2025-08-23 20:55:46] logging.py:143 >> {'loss': 0.7613, 'learning_rate': 2.1977e-05, 'epoch': 4.86, 'throughput': 4645.35}
[INFO|2025-08-23 20:56:26] logging.py:143 >> {'loss': 0.7511, 'learning_rate': 2.1377e-05, 'epoch': 4.93, 'throughput': 4646.69}
[INFO|2025-08-23 20:57:05] logging.py:143 >> {'loss': 0.7763, 'learning_rate': 2.0778e-05, 'epoch': 5.00, 'throughput': 4647.42}
[INFO|2025-08-23 20:57:45] logging.py:143 >> {'loss': 0.6427, 'learning_rate': 2.0182e-05, 'epoch': 5.07, 'throughput': 4646.99}
[INFO|2025-08-23 20:58:27] logging.py:143 >> {'loss': 0.6129, 'learning_rate': 1.9589e-05, 'epoch': 5.14, 'throughput': 4645.61}
[INFO|2025-08-23 20:59:07] logging.py:143 >> {'loss': 0.6337, 'learning_rate': 1.8999e-05, 'epoch': 5.21, 'throughput': 4646.46}
[INFO|2025-08-23 20:59:47] logging.py:143 >> {'loss': 0.6358, 'learning_rate': 1.8413e-05, 'epoch': 5.28, 'throughput': 4647.17}
[INFO|2025-08-23 21:00:27] logging.py:143 >> {'loss': 0.6132, 'learning_rate': 1.7830e-05, 'epoch': 5.35, 'throughput': 4647.83}
[INFO|2025-08-23 21:01:07] logging.py:143 >> {'loss': 0.6229, 'learning_rate': 1.7252e-05, 'epoch': 5.42, 'throughput': 4649.04}
[INFO|2025-08-23 21:01:47] logging.py:143 >> {'loss': 0.6165, 'learning_rate': 1.6678e-05, 'epoch': 5.49, 'throughput': 4649.78}
[INFO|2025-08-23 21:02:27] logging.py:143 >> {'loss': 0.6347, 'learning_rate': 1.6109e-05, 'epoch': 5.56, 'throughput': 4650.19}
[INFO|2025-08-23 21:02:27] trainer.py:4408 >> 
***** Running Evaluation *****
[INFO|2025-08-23 21:02:27] trainer.py:4410 >>   Num examples = 24
[INFO|2025-08-23 21:02:27] trainer.py:4413 >>   Batch size = 1
[INFO|2025-08-23 21:02:31] trainer.py:4074 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400
[INFO|2025-08-23 21:02:31] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 21:02:31] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 21:02:32] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/chat_template.jinja
[INFO|2025-08-23 21:02:32] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/tokenizer_config.json
[INFO|2025-08-23 21:02:32] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/special_tokens_map.json
[INFO|2025-08-23 21:02:34] image_processing_base.py:258 >> Image processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/preprocessor_config.json
[INFO|2025-08-23 21:02:34] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/chat_template.jinja
[INFO|2025-08-23 21:02:34] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/tokenizer_config.json
[INFO|2025-08-23 21:02:34] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/special_tokens_map.json
[INFO|2025-08-23 21:02:34] video_processing_utils.py:582 >> Video processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/video_preprocessor_config.json
[INFO|2025-08-23 21:02:34] processing_utils.py:745 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-400/chat_template.jinja
[INFO|2025-08-23 21:03:14] logging.py:143 >> {'loss': 0.6347, 'learning_rate': 1.5545e-05, 'epoch': 5.63, 'throughput': 4642.44}
[INFO|2025-08-23 21:03:54] logging.py:143 >> {'loss': 0.6335, 'learning_rate': 1.4987e-05, 'epoch': 5.70, 'throughput': 4644.12}
[INFO|2025-08-23 21:04:34] logging.py:143 >> {'loss': 0.6375, 'learning_rate': 1.4435e-05, 'epoch': 5.77, 'throughput': 4644.03}
[INFO|2025-08-23 21:05:14] logging.py:143 >> {'loss': 0.6442, 'learning_rate': 1.3888e-05, 'epoch': 5.84, 'throughput': 4645.46}
[INFO|2025-08-23 21:05:55] logging.py:143 >> {'loss': 0.6445, 'learning_rate': 1.3349e-05, 'epoch': 5.91, 'throughput': 4645.33}
[INFO|2025-08-23 21:06:35] logging.py:143 >> {'loss': 0.6436, 'learning_rate': 1.2816e-05, 'epoch': 5.98, 'throughput': 4645.93}
[INFO|2025-08-23 21:07:13] logging.py:143 >> {'loss': 0.5938, 'learning_rate': 1.2291e-05, 'epoch': 6.04, 'throughput': 4647.22}
[INFO|2025-08-23 21:07:53] logging.py:143 >> {'loss': 0.5625, 'learning_rate': 1.1773e-05, 'epoch': 6.11, 'throughput': 4648.58}
[INFO|2025-08-23 21:08:33] logging.py:143 >> {'loss': 0.5589, 'learning_rate': 1.1262e-05, 'epoch': 6.18, 'throughput': 4648.51}
[INFO|2025-08-23 21:09:14] logging.py:143 >> {'loss': 0.5575, 'learning_rate': 1.0760e-05, 'epoch': 6.25, 'throughput': 4649.67}
[INFO|2025-08-23 21:09:54] logging.py:143 >> {'loss': 0.5458, 'learning_rate': 1.0266e-05, 'epoch': 6.32, 'throughput': 4649.21}
[INFO|2025-08-23 21:10:35] logging.py:143 >> {'loss': 0.5461, 'learning_rate': 9.7810e-06, 'epoch': 6.39, 'throughput': 4649.23}
[INFO|2025-08-23 21:11:14] logging.py:143 >> {'loss': 0.5429, 'learning_rate': 9.3047e-06, 'epoch': 6.46, 'throughput': 4650.80}
[INFO|2025-08-23 21:11:54] logging.py:143 >> {'loss': 0.5483, 'learning_rate': 8.8377e-06, 'epoch': 6.53, 'throughput': 4651.25}
[INFO|2025-08-23 21:12:34] logging.py:143 >> {'loss': 0.5479, 'learning_rate': 8.3801e-06, 'epoch': 6.60, 'throughput': 4651.95}
[INFO|2025-08-23 21:13:13] logging.py:143 >> {'loss': 0.5478, 'learning_rate': 7.9323e-06, 'epoch': 6.67, 'throughput': 4653.19}
[INFO|2025-08-23 21:13:54] logging.py:143 >> {'loss': 0.5557, 'learning_rate': 7.4946e-06, 'epoch': 6.74, 'throughput': 4653.65}
[INFO|2025-08-23 21:14:34] logging.py:143 >> {'loss': 0.5458, 'learning_rate': 7.0671e-06, 'epoch': 6.81, 'throughput': 4653.29}
[INFO|2025-08-23 21:15:15] logging.py:143 >> {'loss': 0.5361, 'learning_rate': 6.6502e-06, 'epoch': 6.88, 'throughput': 4653.54}
[INFO|2025-08-23 21:15:56] logging.py:143 >> {'loss': 0.5571, 'learning_rate': 6.2440e-06, 'epoch': 6.95, 'throughput': 4653.30}
[INFO|2025-08-23 21:15:56] trainer.py:4408 >> 
***** Running Evaluation *****
[INFO|2025-08-23 21:15:56] trainer.py:4410 >>   Num examples = 24
[INFO|2025-08-23 21:15:56] trainer.py:4413 >>   Batch size = 1
[INFO|2025-08-23 21:16:00] trainer.py:4074 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500
[INFO|2025-08-23 21:16:00] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 21:16:00] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 21:16:01] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/chat_template.jinja
[INFO|2025-08-23 21:16:01] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/tokenizer_config.json
[INFO|2025-08-23 21:16:01] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/special_tokens_map.json
[INFO|2025-08-23 21:16:03] image_processing_base.py:258 >> Image processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/preprocessor_config.json
[INFO|2025-08-23 21:16:03] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/chat_template.jinja
[INFO|2025-08-23 21:16:03] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/tokenizer_config.json
[INFO|2025-08-23 21:16:03] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/special_tokens_map.json
[INFO|2025-08-23 21:16:03] video_processing_utils.py:582 >> Video processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/video_preprocessor_config.json
[INFO|2025-08-23 21:16:03] processing_utils.py:745 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-500/chat_template.jinja
[INFO|2025-08-23 21:16:41] logging.py:143 >> {'loss': 0.5381, 'learning_rate': 5.8489e-06, 'epoch': 7.01, 'throughput': 4645.78}
[INFO|2025-08-23 21:17:22] logging.py:143 >> {'loss': 0.4812, 'learning_rate': 5.4650e-06, 'epoch': 7.08, 'throughput': 4645.98}
[INFO|2025-08-23 21:18:03] logging.py:143 >> {'loss': 0.4926, 'learning_rate': 5.0926e-06, 'epoch': 7.15, 'throughput': 4645.50}
[INFO|2025-08-23 21:18:43] logging.py:143 >> {'loss': 0.4931, 'learning_rate': 4.7319e-06, 'epoch': 7.22, 'throughput': 4645.88}
[INFO|2025-08-23 21:19:23] logging.py:143 >> {'loss': 0.5112, 'learning_rate': 4.3831e-06, 'epoch': 7.29, 'throughput': 4646.53}
[INFO|2025-08-23 21:20:05] logging.py:143 >> {'loss': 0.4957, 'learning_rate': 4.0464e-06, 'epoch': 7.36, 'throughput': 4645.29}
[INFO|2025-08-23 21:20:46] logging.py:143 >> {'loss': 0.5062, 'learning_rate': 3.7221e-06, 'epoch': 7.43, 'throughput': 4644.89}
[INFO|2025-08-23 21:21:26] logging.py:143 >> {'loss': 0.4868, 'learning_rate': 3.4102e-06, 'epoch': 7.50, 'throughput': 4645.52}
[INFO|2025-08-23 21:22:06] logging.py:143 >> {'loss': 0.5105, 'learning_rate': 3.1110e-06, 'epoch': 7.57, 'throughput': 4645.74}
[INFO|2025-08-23 21:22:45] logging.py:143 >> {'loss': 0.4831, 'learning_rate': 2.8247e-06, 'epoch': 7.64, 'throughput': 4646.68}
[INFO|2025-08-23 21:23:25] logging.py:143 >> {'loss': 0.5094, 'learning_rate': 2.5514e-06, 'epoch': 7.71, 'throughput': 4647.99}
[INFO|2025-08-23 21:24:05] logging.py:143 >> {'loss': 0.4941, 'learning_rate': 2.2913e-06, 'epoch': 7.78, 'throughput': 4648.56}
[INFO|2025-08-23 21:24:46] logging.py:143 >> {'loss': 0.4992, 'learning_rate': 2.0446e-06, 'epoch': 7.85, 'throughput': 4648.94}
[INFO|2025-08-23 21:25:26] logging.py:143 >> {'loss': 0.4845, 'learning_rate': 1.8113e-06, 'epoch': 7.92, 'throughput': 4649.83}
[INFO|2025-08-23 21:26:05] logging.py:143 >> {'loss': 0.5132, 'learning_rate': 1.5917e-06, 'epoch': 7.99, 'throughput': 4651.25}
[INFO|2025-08-23 21:26:44] logging.py:143 >> {'loss': 0.4715, 'learning_rate': 1.3858e-06, 'epoch': 8.06, 'throughput': 4650.79}
[INFO|2025-08-23 21:27:24] logging.py:143 >> {'loss': 0.4767, 'learning_rate': 1.1938e-06, 'epoch': 8.13, 'throughput': 4651.70}
[INFO|2025-08-23 21:28:04] logging.py:143 >> {'loss': 0.4831, 'learning_rate': 1.0158e-06, 'epoch': 8.20, 'throughput': 4652.59}
[INFO|2025-08-23 21:28:44] logging.py:143 >> {'loss': 0.4722, 'learning_rate': 8.5185e-07, 'epoch': 8.26, 'throughput': 4652.62}
[INFO|2025-08-23 21:29:24] logging.py:143 >> {'loss': 0.4701, 'learning_rate': 7.0212e-07, 'epoch': 8.33, 'throughput': 4653.54}
[INFO|2025-08-23 21:29:24] trainer.py:4408 >> 
***** Running Evaluation *****
[INFO|2025-08-23 21:29:24] trainer.py:4410 >>   Num examples = 24
[INFO|2025-08-23 21:29:24] trainer.py:4413 >>   Batch size = 1
[INFO|2025-08-23 21:29:28] trainer.py:4074 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600
[INFO|2025-08-23 21:29:28] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 21:29:28] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 21:29:29] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/chat_template.jinja
[INFO|2025-08-23 21:29:29] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/tokenizer_config.json
[INFO|2025-08-23 21:29:29] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/special_tokens_map.json
[INFO|2025-08-23 21:29:30] image_processing_base.py:258 >> Image processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/preprocessor_config.json
[INFO|2025-08-23 21:29:30] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/chat_template.jinja
[INFO|2025-08-23 21:29:30] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/tokenizer_config.json
[INFO|2025-08-23 21:29:30] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/special_tokens_map.json
[INFO|2025-08-23 21:29:30] video_processing_utils.py:582 >> Video processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/video_preprocessor_config.json
[INFO|2025-08-23 21:29:31] processing_utils.py:745 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-600/chat_template.jinja
[INFO|2025-08-23 21:30:11] logging.py:143 >> {'loss': 0.4769, 'learning_rate': 5.6665e-07, 'epoch': 8.40, 'throughput': 4647.55}
[INFO|2025-08-23 21:30:51] logging.py:143 >> {'loss': 0.4655, 'learning_rate': 4.4555e-07, 'epoch': 8.47, 'throughput': 4648.51}
[INFO|2025-08-23 21:31:32] logging.py:143 >> {'loss': 0.4579, 'learning_rate': 3.3887e-07, 'epoch': 8.54, 'throughput': 4648.40}
[INFO|2025-08-23 21:32:12] logging.py:143 >> {'loss': 0.4825, 'learning_rate': 2.4668e-07, 'epoch': 8.61, 'throughput': 4649.54}
[INFO|2025-08-23 21:32:52] logging.py:143 >> {'loss': 0.4785, 'learning_rate': 1.6904e-07, 'epoch': 8.68, 'throughput': 4649.43}
[INFO|2025-08-23 21:33:32] logging.py:143 >> {'loss': 0.4735, 'learning_rate': 1.0599e-07, 'epoch': 8.75, 'throughput': 4649.69}
[INFO|2025-08-23 21:34:12] logging.py:143 >> {'loss': 0.4820, 'learning_rate': 5.7564e-08, 'epoch': 8.82, 'throughput': 4650.78}
[INFO|2025-08-23 21:34:52] logging.py:143 >> {'loss': 0.4751, 'learning_rate': 2.3794e-08, 'epoch': 8.89, 'throughput': 4650.38}
[INFO|2025-08-23 21:35:33] logging.py:143 >> {'loss': 0.4716, 'learning_rate': 4.7007e-09, 'epoch': 8.96, 'throughput': 4650.43}
[INFO|2025-08-23 21:35:55] trainer.py:4074 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648
[INFO|2025-08-23 21:35:55] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 21:35:55] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 21:35:56] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/chat_template.jinja
[INFO|2025-08-23 21:35:56] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/tokenizer_config.json
[INFO|2025-08-23 21:35:56] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/special_tokens_map.json
[INFO|2025-08-23 21:35:58] image_processing_base.py:258 >> Image processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/preprocessor_config.json
[INFO|2025-08-23 21:35:58] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/chat_template.jinja
[INFO|2025-08-23 21:35:58] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/tokenizer_config.json
[INFO|2025-08-23 21:35:58] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/special_tokens_map.json
[INFO|2025-08-23 21:35:58] video_processing_utils.py:582 >> Video processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/video_preprocessor_config.json
[INFO|2025-08-23 21:35:58] processing_utils.py:745 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/checkpoint-648/chat_template.jinja
[INFO|2025-08-23 21:35:58] trainer.py:2718 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|2025-08-23 21:35:58] image_processing_base.py:258 >> Image processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/preprocessor_config.json
[INFO|2025-08-23 21:35:58] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/chat_template.jinja
[INFO|2025-08-23 21:35:58] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/tokenizer_config.json
[INFO|2025-08-23 21:35:58] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/special_tokens_map.json
[INFO|2025-08-23 21:35:59] video_processing_utils.py:582 >> Video processor saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/video_preprocessor_config.json
[INFO|2025-08-23 21:35:59] processing_utils.py:745 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/chat_template.jinja
[INFO|2025-08-23 21:35:59] trainer.py:4074 >> Saving model checkpoint to saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9
[INFO|2025-08-23 21:35:59] configuration_utils.py:750 >> loading configuration file /data/wtt/wjx_code/VLM/Qwen2.5-VL-7B-Instruct/config.json
[INFO|2025-08-23 21:35:59] configuration_utils.py:817 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "image_token_id": null,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "use_sliding_window": false,
    "video_token_id": null,
    "vision_end_token_id": 151653,
    "vision_start_token_id": 151652,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-08-23 21:36:00] tokenization_utils_base.py:2393 >> chat template saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/chat_template.jinja
[INFO|2025-08-23 21:36:00] tokenization_utils_base.py:2562 >> tokenizer config file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/tokenizer_config.json
[INFO|2025-08-23 21:36:00] tokenization_utils_base.py:2571 >> Special tokens file saved in saves/Qwen2.5-7B-Instruct/lora/Qwen2.5-VL-7B-Instruct-Epoch=9/special_tokens_map.json
[WARNING|2025-08-23 21:36:01] logging.py:148 >> No metric eval_accuracy to plot.
[INFO|2025-08-23 21:36:01] trainer.py:4408 >> 
***** Running Evaluation *****
[INFO|2025-08-23 21:36:01] trainer.py:4410 >>   Num examples = 24
[INFO|2025-08-23 21:36:01] trainer.py:4413 >>   Batch size = 1
[INFO|2025-08-23 21:36:04] modelcard.py:456 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
